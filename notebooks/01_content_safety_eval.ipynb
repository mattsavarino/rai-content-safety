{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content Safety Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from typing import Optional, List, Dict, Any\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from promptflow.evals.synthetic import AdversarialSimulator, AdversarialScenario\n",
    "from promptflow.evals.evaluators import (ContentSafetyEvaluator)\n",
    "from promptflow.evals.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "SUBSCRIPTION_ID = os.environ.get('SUBSCRIPTION_ID') \n",
    "RESOURCE_GROUP = os.environ.get('RESOURCE_GROUP') \n",
    "AI_PROJECT_NAME = os.environ.get('AI_PROJECT_NAME') \n",
    "\n",
    "# Define Azure AI Studio project\n",
    "try:\n",
    "  azure_ai_project = {\n",
    "    \"subscription_id\": SUBSCRIPTION_ID,\n",
    "    \"resource_group_name\": RESOURCE_GROUP,\n",
    "    \"project_name\": AI_PROJECT_NAME,\n",
    "    \"credential\": DefaultAzureCredential(),\n",
    "  }\n",
    "except Exception as e:\n",
    "  print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate environment variables\n",
    "print(f\"\"\"\n",
    "SUBSCRIPTION_ID: {SUBSCRIPTION_ID}\n",
    "RESOURCE_GROUP: {RESOURCE_GROUP}\n",
    "AI_PROJECT_NAME: {AI_PROJECT_NAME}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize simulator\n",
    "simulator = AdversarialSimulator(azure_ai_project=azure_ai_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Functions\n",
    "\n",
    "**Note:** This code block should be updated to call your API endpoint and to handle its response structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to call API endpoint\n",
    "def call_endpoint(query: str) -> dict:\n",
    "  # Placeholder API response from endpoint\n",
    "  return {\n",
    "    \"answer\": \"This is a test response.\",\n",
    "    \"context\": \"\"\n",
    "  }\n",
    "\n",
    "  # Custom API endpoint\n",
    "  # api_path = \"YOUR_API_ENDPOINT\"\n",
    "  # json_payload = {}\n",
    "  # response = requests.post(api_path, json=json_payload)\n",
    "  # response_json = response.json()\n",
    "  # answer = response_json[\"text\"]\n",
    "  # json_response = {\n",
    "  #   \"answer\": answer,\n",
    "  #   \"context\": \"\"\n",
    "  # }\n",
    "  # return json_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test endpoint function\n",
    "call_endpoint(\"Hello, what can you help with?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for simulation callback\n",
    "async def simulation_callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    response_from_api = call_endpoint(query)\n",
    "    # Format response to OpenAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response_from_api[\"answer\"],\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": response_from_api[\"context\"],\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run simulation with callback\n",
    "async def run_simulation(\n",
    "    output_path: str,\n",
    "    max_simulation_results: int = 10,\n",
    "    jailbreak: bool = False,\n",
    "):\n",
    "  try:\n",
    "    print(\"Running simulation...\")\n",
    "    sim_output = await simulator(\n",
    "      target=simulation_callback,\n",
    "      scenario=AdversarialScenario.ADVERSARIAL_QA, \n",
    "      max_conversation_turns=1, \n",
    "      max_simulation_results=max_simulation_results, \n",
    "      jailbreak=jailbreak\n",
    "    )\n",
    "    with Path.open(output_path, \"w\") as f:\n",
    "      f.write(sim_output.to_eval_qa_json_lines())\n",
    "    print(f\"Simulation results written to {output_path}\")\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulations\n",
    "Generated files written to [../data/](../data/)\n",
    "* `YYYYMMDDHHDDSS_nonjailbreak_sim.jsonl`\n",
    "* `YYYYMMDDHHDDSS_jailbreak_sim.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define common params\n",
    "max_simulation_results = 10\n",
    "file_prefix = f\"../data/{datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Jailbreak Simulation\n",
    "file_nonjailbreak_sim = f\"{file_prefix}_nonjailbreak_sim.jsonl\"\n",
    "sim_results = await run_simulation(\n",
    "  output_path=file_nonjailbreak_sim,\n",
    "  max_simulation_results=max_simulation_results,\n",
    "  jailbreak=False # FALSE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jailbreak Simulation\n",
    "file_jailbreak_sim = f\"{file_prefix}_jailbreak_sim.jsonl\"\n",
    "sim_results = await run_simulation(\n",
    "  output_path=file_jailbreak_sim,\n",
    "  max_simulation_results=max_simulation_results,\n",
    "  jailbreak=True # TRUE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluators\n",
    "Generated files written to [../data/](../data/)\n",
    "* `YYYYMMDDHHDDSS_nonjailbreak_eval.jsonl`\n",
    "* `YYYYMMDDHHDDSS_jailbreak_eval.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credential is acquired within evaluate\n",
    "# Note: When credential is passed, evaluate() function may return odd pickle error\n",
    "azure_ai_project.pop(\"credential\", None)\n",
    "\n",
    "# Initialize Azure Content Safety Evaluator\n",
    "content_safety_evaluator = ContentSafetyEvaluator(project_scope=azure_ai_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run evaluation\n",
    "async def run_evaluation(\n",
    "    input_sim_data: str,\n",
    "    output_file_name: str\n",
    "):\n",
    "  eval_results = evaluate(\n",
    "    data=input_sim_data,\n",
    "    evaluators={\"content_safety\": content_safety_evaluator}\n",
    "  )\n",
    "\n",
    "  # Write non-jailbreak evaluation results\n",
    "  try:\n",
    "    file_eval = f\"{file_prefix}_{output_file_name}_eval.jsonl\"\n",
    "    with Path(file_eval).open(\"w\") as f:\n",
    "      json_string = json.dumps(eval_results)\n",
    "      f.write(json_string)\n",
    "    print(f\"Wrote eval file to {file_eval}\")\n",
    "    return eval_results\n",
    "  except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Jailbreak Evaluation\n",
    "eval_results_nonjailbreak = await run_evaluation(\n",
    "  input_sim_data = file_nonjailbreak_sim,\n",
    "  output_file_name = \"nonjailbreak\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jailbreak Evaluation\n",
    "eval_results_jailbreak = await run_evaluation(\n",
    "  input_sim_data = file_jailbreak_sim,\n",
    "  output_file_name = \"jailbreak\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Non-Jailbreak Evaluation:\\n\")\n",
    "for metric in eval_results_nonjailbreak['metrics']:\n",
    "  print(f\"{eval_results_nonjailbreak['metrics'][metric]} = {metric}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Jailbreak Evaluation:\\n\")\n",
    "for metric in eval_results_jailbreak['metrics']:\n",
    "  print(f\"{eval_results_jailbreak['metrics'][metric]} = {metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Files\n",
    "Check the [data directory](../data/) for generated JSONL files.\n",
    "\n",
    "Note a traces report gets created on localhost and linked from `run_evaluation()` response above.\n",
    "\n",
    "The url will be similar to:<br>`http://127.0.0.1:23335/v1.0/ui/traces/?...`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
